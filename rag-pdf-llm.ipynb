{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7800136,"sourceType":"datasetVersion","datasetId":4567161},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-12T17:37:05.270306Z","iopub.execute_input":"2024-03-12T17:37:05.270562Z","iopub.status.idle":"2024-03-12T17:37:06.383366Z","shell.execute_reply.started":"2024-03-12T17:37:05.270538Z","shell.execute_reply":"2024-03-12T17:37:06.382336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyPDF2 InstructorEmbedding langchain langchain-community langchainhub langchain-mistralai transformers sentence-transformers faiss-gpu\n!pip install -q -U bitsandbytes\n!pip install -q -U peft\n!pip install -q -U accelerate","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:37:06.385102Z","iopub.execute_input":"2024-03-12T17:37:06.385513Z","iopub.status.idle":"2024-03-12T17:38:49.236258Z","shell.execute_reply.started":"2024-03-12T17:37:06.385487Z","shell.execute_reply":"2024-03-12T17:38:49.235185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastapi\n!pip install uvicorn\n!pip install pyngrok\n!pip install nest_asyncio\n!pip install python-multipart","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:38:49.237907Z","iopub.execute_input":"2024-03-12T17:38:49.238295Z","iopub.status.idle":"2024-03-12T17:39:55.349004Z","shell.execute_reply.started":"2024-03-12T17:38:49.238262Z","shell.execute_reply":"2024-03-12T17:39:55.347848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ngrok config add-authtoken 2c2PwQHIfIGaGan2nGOCD7Hau02_m9sPZNHgzv2T4mmrhr1a","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:39:55.351717Z","iopub.execute_input":"2024-03-12T17:39:55.352034Z","iopub.status.idle":"2024-03-12T17:39:57.103144Z","shell.execute_reply.started":"2024-03-12T17:39:55.352007Z","shell.execute_reply":"2024-03-12T17:39:57.102148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PyPDF2 as pdf\nfrom langchain_mistralai.chat_models import ChatMistralAI\nfrom langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, ConversationChain\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFacePipeline\nimport transformers\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom langchain_core.runnables import RunnablePassthrough","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:39:57.104484Z","iopub.execute_input":"2024-03-12T17:39:57.104773Z","iopub.status.idle":"2024-03-12T17:40:06.591603Z","shell.execute_reply.started":"2024-03-12T17:39:57.104749Z","shell.execute_reply":"2024-03-12T17:40:06.590797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initializing Model and Tokenizer for future use","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\", quantization_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n))\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:40:06.592757Z","iopub.execute_input":"2024-03-12T17:40:06.593307Z","iopub.status.idle":"2024-03-12T17:41:49.50791Z","shell.execute_reply.started":"2024-03-12T17:40:06.59328Z","shell.execute_reply":"2024-03-12T17:41:49.506868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to: Generate Documents from pdf","metadata":{}},{"cell_type":"code","source":"def extract_documents_from_pdf(path, chunk_size = 1000, chunk_overlap = 200):\n    pdf_file = pdf.PdfReader(path)\n    paper = \"\"\n    for i in range(pdf_file._get_num_pages()):\n        paper += pdf_file._get_page(i).extract_text()\n    paper = paper.split(\"\\n\")\n    paper = \"\".join(paper)\n    splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n    documents = splitter.create_documents([paper])\n    return documents","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.509212Z","iopub.execute_input":"2024-03-12T17:41:49.509731Z","iopub.status.idle":"2024-03-12T17:41:49.516171Z","shell.execute_reply.started":"2024-03-12T17:41:49.509694Z","shell.execute_reply":"2024-03-12T17:41:49.515236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to: Create Vectors from the Documents and returns a retriver","metadata":{}},{"cell_type":"code","source":"def get_retriever(documents):\n    embeddings = HuggingFaceEmbeddings()\n    str_list = [x.__str__() for x in documents]\n    vectors = embeddings.embed_documents(str_list)\n    db = FAISS.from_documents(documents, embeddings)\n    return db.as_retriever(search_kwargs={\"k\": 4})","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.517415Z","iopub.execute_input":"2024-03-12T17:41:49.517694Z","iopub.status.idle":"2024-03-12T17:41:49.529185Z","shell.execute_reply.started":"2024-03-12T17:41:49.517662Z","shell.execute_reply":"2024-03-12T17:41:49.528309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to: Create an LLM chain that answers to the query","metadata":{}},{"cell_type":"code","source":"def create_llm_chain(retriever, model, tokenizer):\n    text_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=800,\n    )\n    \n    \n    prompt_template = \"\"\"\n### [INST] You are an AI assistant specialized in analyzing and answering questions based on academic papers. You have access to a large database of research papers spanning various fields. Please provide a detailed answer to the question based on the given context. If the context is not sufficient to answer the question, kindly let me know that additional information is needed, and I will try to provide more relevant context from the research paper database. Here is the context:\n{context}\n\n### QUESTION:\n{question}\n\n[/INST]\n\"\"\"\n#    prompt_template = \"\"\"\n#### [INST] \n#Instruction: Answer the question based with the help of following context:\n#{context}\n#\n#### QUESTION:\n#{question} \n#\n#[/INST]\n#\"\"\"\n    mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n    prompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n    )\n\n    # Create llm chain \n    llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n    return llm_chain","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.530529Z","iopub.execute_input":"2024-03-12T17:41:49.531166Z","iopub.status.idle":"2024-03-12T17:41:49.541822Z","shell.execute_reply.started":"2024-03-12T17:41:49.53113Z","shell.execute_reply":"2024-03-12T17:41:49.540863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to: Create a Chain that helps query the datastore for the relevant documents","metadata":{}},{"cell_type":"code","source":"def create_rag_chain(retriever, llm_chain):\n    rag_chain = ( \n     {\"context\": retriever, \"question\": RunnablePassthrough()}\n        | llm_chain\n    )\n\n    return rag_chain","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.545425Z","iopub.execute_input":"2024-03-12T17:41:49.545793Z","iopub.status.idle":"2024-03-12T17:41:49.556253Z","shell.execute_reply.started":"2024-03-12T17:41:49.545758Z","shell.execute_reply":"2024-03-12T17:41:49.555315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to: Use all the previous functions generated and create a LLM chain","metadata":{}},{"cell_type":"code","source":"def create_llm(path, model, tokenizer, chunk_size = 1000, chunk_overlap = 200):\n    documents = extract_documents_from_pdf(path, chunk_size, chunk_overlap)\n    retriever = get_retriever(documents)\n    llm_chain = create_llm_chain(retriever, model, tokenizer)\n    rag_chain = create_rag_chain(retriever, llm_chain)\n    return rag_chain\n\nllm = None","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.557597Z","iopub.execute_input":"2024-03-12T17:41:49.558081Z","iopub.status.idle":"2024-03-12T17:41:49.567454Z","shell.execute_reply.started":"2024-03-12T17:41:49.558045Z","shell.execute_reply":"2024-03-12T17:41:49.56644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/fake-med-det-paper/fake_med_1.pdf\"\n#llm = create_llm(path, model, tokenizer, 1000, 200)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.568669Z","iopub.execute_input":"2024-03-12T17:41:49.568961Z","iopub.status.idle":"2024-03-12T17:41:49.577477Z","shell.execute_reply.started":"2024-03-12T17:41:49.568938Z","shell.execute_reply":"2024-03-12T17:41:49.576427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#llm.invoke(\"did they mention in the paper Counterfeit Medicine Detection using Deep Learning how does object detection work for logo detection?\").get(\"text\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.578639Z","iopub.execute_input":"2024-03-12T17:41:49.579212Z","iopub.status.idle":"2024-03-12T17:41:49.587449Z","shell.execute_reply.started":"2024-03-12T17:41:49.579187Z","shell.execute_reply":"2024-03-12T17:41:49.586517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summarize(text):\n    text_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.4,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=600,\n    )\n    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)   \n    ans = llm.predict(\n    f'''\n    ### [INST] \n    Instruction: Summarize the following piece of information and cover only the important points, try keeping it concise and small and return the response in paragraph format:\n\n    ### QUESTION:\n    {text} \n\n    [/INST]\n    '''\n    )\n    return \"\".join(ans.split(\"\\n\")).strip()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.588524Z","iopub.execute_input":"2024-03-12T17:41:49.588802Z","iopub.status.idle":"2024-03-12T17:41:49.601239Z","shell.execute_reply.started":"2024-03-12T17:41:49.58878Z","shell.execute_reply":"2024-03-12T17:41:49.60037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the LLM","metadata":{}},{"cell_type":"code","source":"text = '''Czech Republic’s Krystyna Pyszkova on Saturday won the Miss World 2024 title at a grand event in Mumbai. Reigning Miss World Karolina Bielawska from Poland crowned her successor at the star-studded finale here.\nApart from studying for dual degrees in Law and Business Administration, Krystyna also works as a model. She established the Krystyna Pyszko Foundation and remains actively involved in its initiatives, according to the Miss World website.\nAfter Pyszkova, Miss Lebanon Yasmina Zaytoun was named the first runner-up. \nIndia, which hosted the event after 28 years, was represented by 22-year-old Sini Shetty. Mumbai-born Shetty, who was crowned Femina Miss India World in 2022, was unable to make it to the top 4 of the contest. India has won the prestigious title six times — Reita Faria (1966), Aishwarya Rai Bachchan (1994), Diana Hayden (1997), Yukta Mookhey (1999), Priyanka Chopra Jonas (2000), and Manushi Chillar (2017). The 71st Miss World pageant, which witnessed the participation of contestants from 112 countries of the world, was held at the Jio World Convention Centre in BKC here.\nPart of the 12-judge panel for the finale were film producer Sajid Nadiadwala; actors Kriti Sanon, Pooja Hegde; cricketer Harbhajan Singh; news personality Rajat Sharma, social worker Amruta Fadnavis; Vineet Jain, MD of Bennett, Coleman & Co. Limited; Julia Morley, Chairperson and CEO of the Miss World Organization; Jamil Saidi, Strategic Partner & Host – Miss World India, and three former Miss Worlds, including Chillar.\nFilmmaker Karan Johar and former Miss World Megan Young hosted the event, which kickstarted on a high note with performances by singers Shaan, Neha Kakkar, and Tony Kakkar. A video message by Chopra Jonas highlighting the importance of ’beauty with purpose’, a tagline associated with the Miss World pageant, was also played at the event.\nThe cast of Sanjay Leela Bhansali’s maiden web series Heeramandi: The Diamond Bazaar” — Manisha Koirala, Sonakshi Sinha, Aditi Rao Hydari, Richa Chadha, Sharmin Segal, and Sanjeeda Sheikh — also walked the stage with 13 fast-track Miss World contestants on the show’s newly released song “Sakal Ban”. The month-long Miss World event featured a series of rigorous competitions, including talent showcases, sports challenges, and charitable initiatives — all aimed at highlighting the qualities that make these competitors the ambassadors of change.\n'''\ntext2 = '''\nDiff types of dataset used in ML\nIn machine learning, there are several types of datasets commonly used for training and evaluating models:\nTraining Dataset: This dataset is used to train the machine learning model. It consists of a set of input-output pairs that the model learns from during the training process.\nValidation Dataset: This dataset is used to tune the hyperparameters of the model and to evaluate its performance during training. It helps prevent overfitting by providing an independent dataset that the model hasn't seen during training.\nTest Dataset: The test dataset is used to evaluate the final performance of the trained model. It serves as an unseen dataset to assess how well the model generalizes to new, unseen data.\nCross-Validation Dataset: In k-fold cross-validation, the dataset is divided into k subsets. The model is trained and evaluated k times, each time using a different subset as the validation set and the remaining data for training. This helps provide a more reliable estimate of the model's performance.\nThese are some of the common types of datasets used in machine learning, each tailored to different types of tasks and data structures.\n'''\nlen(text2)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.602446Z","iopub.execute_input":"2024-03-12T17:41:49.6027Z","iopub.status.idle":"2024-03-12T17:41:49.615669Z","shell.execute_reply.started":"2024-03-12T17:41:49.602673Z","shell.execute_reply":"2024-03-12T17:41:49.614834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = summarize(text2)\nprint(len(ans))\nans","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:41:49.616936Z","iopub.execute_input":"2024-03-12T17:41:49.617286Z","iopub.status.idle":"2024-03-12T17:42:14.992871Z","shell.execute_reply.started":"2024-03-12T17:41:49.617254Z","shell.execute_reply":"2024-03-12T17:42:14.99185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a FastAPI Server","metadata":{}},{"cell_type":"code","source":"from fastapi import FastAPI, Request, File, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\nimport nest_asyncio\nfrom pyngrok import ngrok","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:14.994356Z","iopub.execute_input":"2024-03-12T17:42:14.995157Z","iopub.status.idle":"2024-03-12T17:42:15.764752Z","shell.execute_reply.started":"2024-03-12T17:42:14.995118Z","shell.execute_reply":"2024-03-12T17:42:15.764008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app = FastAPI()\norigins = [\"*\"]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.765776Z","iopub.execute_input":"2024-03-12T17:42:15.766113Z","iopub.status.idle":"2024-03-12T17:42:15.771161Z","shell.execute_reply.started":"2024-03-12T17:42:15.766087Z","shell.execute_reply":"2024-03-12T17:42:15.770455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UPLOAD_DIRECTORY = \"/kaggle/working/pdfs\"  # Directory where PDF files will be saved\n\n# Create the upload directory if it doesn't exist\nos.makedirs(UPLOAD_DIRECTORY, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.772509Z","iopub.execute_input":"2024-03-12T17:42:15.77276Z","iopub.status.idle":"2024-03-12T17:42:15.784332Z","shell.execute_reply.started":"2024-03-12T17:42:15.772738Z","shell.execute_reply":"2024-03-12T17:42:15.783531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n#get the input from the request body and return the response\n@app.post(\"/summarize/\")\nasync def get_summary(request: Request):\n    data = await request.json()\n    input_text = data['text']\n    if len(input_text) == 0:\n        return {\"error\": \"Input text is empty\"}\n    else:\n        return {\"summary\": summarize(input_text)}","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.785387Z","iopub.execute_input":"2024-03-12T17:42:15.785651Z","iopub.status.idle":"2024-03-12T17:42:15.800126Z","shell.execute_reply.started":"2024-03-12T17:42:15.785627Z","shell.execute_reply":"2024-03-12T17:42:15.799145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@app.post(\"/upload/\")\nasync def upload_file(file: UploadFile = File(...)):\n    global llm\n    with open(os.path.join(UPLOAD_DIRECTORY, file.filename), \"wb\") as buffer:\n        buffer.write(await file.read())\n    llm = create_llm(\"/kaggle/working/pdfs/\"+file.filename, model, tokenizer, 1000, 200)\n    return {\"res\":\"Created and Linked PDF with LLM\"}\n    \n#get the input from the request body and return the response\n@app.post(\"/query-pdf/\")\nasync def queryPDF(request: Request):\n    global llm\n    data = await request.json()\n    input_text = data['text']\n    res = llm.invoke(input_text).get(\"text\")\n    print(res)\n    return {\"res\":res}","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.80129Z","iopub.execute_input":"2024-03-12T17:42:15.801537Z","iopub.status.idle":"2024-03-12T17:42:15.814682Z","shell.execute_reply.started":"2024-03-12T17:42:15.801515Z","shell.execute_reply":"2024-03-12T17:42:15.813858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.815865Z","iopub.execute_input":"2024-03-12T17:42:15.816447Z","iopub.status.idle":"2024-03-12T17:42:15.823719Z","shell.execute_reply.started":"2024-03-12T17:42:15.816404Z","shell.execute_reply":"2024-03-12T17:42:15.822862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/pdfs\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.825011Z","iopub.execute_input":"2024-03-12T17:42:15.825553Z","iopub.status.idle":"2024-03-12T17:42:15.835556Z","shell.execute_reply.started":"2024-03-12T17:42:15.825506Z","shell.execute_reply":"2024-03-12T17:42:15.834631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ngrok_tunnel = ngrok.connect(8000)\nprint(\"Public Url: \"+ngrok_tunnel.public_url)\nnest_asyncio.apply()\nuvicorn.run(app, port=8000)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T17:42:15.836781Z","iopub.execute_input":"2024-03-12T17:42:15.837559Z","iopub.status.idle":"2024-03-12T18:27:09.668263Z","shell.execute_reply.started":"2024-03-12T17:42:15.837527Z","shell.execute_reply":"2024-03-12T18:27:09.667368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}